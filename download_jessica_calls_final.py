#!/usr/bin/env python3

import os
import time
import requests
import pickle
from datetime import datetime
from google.auth.transport.requests import Request
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build

# ---------------- –ù–ê–°–¢–†–û–ô–ö–ò ---------------- #
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
API_KEY = "sk_91b455debc341646af393b6582573e06c70458ce8c0e51d4"
DOC_ID = "1iFo9n49wVAhYfdHQVBzypcm-SzuyY0DCqqpt6Ko4fM4"
PAGE_SIZE = 100
LAST_RUN_FILE = os.path.join(BASE_DIR, "last_run.txt")
CREDENTIALS = os.path.join(BASE_DIR, "credentials.json")
SCOPES = ["https://www.googleapis.com/auth/documents", "https://www.googleapis.com/auth/drive.file"]
TZ_OFFSET_HOURS = 4
AGENT_ID_FILTER = "Ett5Z2WyqmkwilmtCCYJ"
CHUNK_SIZE = 100000

# ---------------- AUTH ---------------- #
def get_credentials():
    creds = None
    token_path = os.path.join(BASE_DIR, "token.pickle")
    if os.path.exists(token_path):
        with open(token_path, "rb") as f:
            creds = pickle.load(f)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS, SCOPES)
            creds = flow.run_local_server(port=0, access_type="offline", include_granted_scopes=True)
        with open(token_path, "wb") as f:
            pickle.dump(creds, f)
    return creds

creds = get_credentials()
docs_service = build("docs", "v1", credentials=creds)

session = requests.Session()
session.headers.update({"xi-api-key": API_KEY, "Accept": "application/json"})

# ---------------- –ó–í–û–ù–ö–ò ---------------- #
def fetch_all_calls():
    url = "https://api.elevenlabs.io/v1/convai/conversations"
    params = {"page_size": PAGE_SIZE}
    all_calls = []
    while True:
        r = session.get(url, params=params)
        r.raise_for_status()
        data = r.json()
        calls = data.get("conversations", [])
        for call in calls:
            if call.get("agent_id") == AGENT_ID_FILTER:
                all_calls.append(call)
        if not data.get("has_more"):
            break
        params["cursor"] = data.get("next_cursor")
    return all_calls

def fetch_call_detail(cid):
    try:
        r = session.get(f"https://api.elevenlabs.io/v1/convai/conversations/{cid}", timeout=10)
        r.raise_for_status()
        return r.json()
    except requests.exceptions.Timeout:
        print(f"‚è± –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∑–≤–æ–Ω–∫–∞ {cid} ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return {}
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∑–≤–æ–Ω–∫–∞ {cid}: {e}")
        return {}

# ---------------- –£–¢–ò–õ–ò–¢–´ ---------------- #
def load_last_run():
    if os.path.exists(LAST_RUN_FILE):
        try:
            with open(LAST_RUN_FILE, "r") as f:
                return int(f.read().strip())
        except:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è last_run.txt")
    return 0

def save_last_run(ts):
    try:
        with open(LAST_RUN_FILE, "w") as f:
            f.write(str(int(ts)))
        print(f"üíæ last_run.txt –æ–±–Ω–æ–≤–ª—ë–Ω: {ts}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ last_run.txt: {e}")

def format_call(detail, fallback_ts):
    ts = detail.get("metadata", {}).get("start_time_unix_secs") or fallback_ts
    adjusted = ts + (TZ_OFFSET_HOURS * 3600)
    dt_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(adjusted))

    summary = (detail.get("analysis") or {}).get("transcript_summary", "").strip()
    transcript = detail.get("transcript", [])

    header = f"=== Call at {dt_str} ===\n"
    if summary:
        header += f"Summary:\n{summary}\n"
    lines = []
    prev_role = None
    for msg in transcript:
        role = (msg.get("role") or "").upper()
        text = (msg.get("message") or "").strip()
        if not text:
            continue
        t = msg.get("time_in_call_secs", 0.0)
        line = f"[{t:06.2f}s] {role}: {text}"
        if prev_role and prev_role != role:
            lines.append("")
        if prev_role == role:
            lines[-1] += "\n" + line
        else:
            lines.append(line)
        prev_role = role
    return header + "\n" + "\n".join(lines) + "\n\n" + "‚Äï" * 40 + "\n\n"

# ---------------- –û–°–ù–û–í–ê ---------------- #
def main():
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç –∑–≤–æ–Ω–∫–æ–≤")
    calls = fetch_all_calls()
    print(f"üì¶ –í—Å–µ–≥–æ –∑–≤–æ–Ω–∫–æ–≤ –æ—Ç –∞–≥–µ–Ω—Ç–∞ {AGENT_ID_FILTER}: {len(calls)}")

    last_ts = load_last_run()
    print(f"‚è± –ü–æ—Å–ª–µ–¥–Ω–∏–π —ç–∫—Å–ø–æ—Ä—Ç: {last_ts} ({datetime.utcfromtimestamp(last_ts)})")

    new_calls = []
    for call in calls:
        ts = call.get("start_time_unix_secs", 0)
        print(f"‚Üí –ó–≤–æ–Ω–æ–∫: {ts} | {call.get('conversation_id')}")
        if ts > last_ts:
            new_calls.append(call)

    if not new_calls:
        print("üîï –ù–µ—Ç –Ω–æ–≤—ã—Ö –∑–≤–æ–Ω–∫–æ–≤ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏")
        return

    new_calls.sort(key=lambda c: c.get("start_time_unix_secs", 0))
    full_text = ""
    max_ts = last_ts

    for call in new_calls:
        cid = call["conversation_id"]
        fallback_ts = call.get("start_time_unix_secs", 0)
        detail = fetch_call_detail(cid)
        time.sleep(0.5)
        if not detail:
            continue
        block = format_call(detail, fallback_ts)
        full_text += block
        ts = detail.get("metadata", {}).get("start_time_unix_secs") or fallback_ts
        if ts > max_ts:
            max_ts = ts

    if not full_text.strip():
        print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç ‚Äî –Ω–∏—á–µ–≥–æ –Ω–µ –≤—Å—Ç–∞–≤–ª—è–µ–º")
        return

    try:
        chunks = [full_text[i:i + CHUNK_SIZE] for i in range(0, len(full_text), CHUNK_SIZE)]
        print(f"‚úÇÔ∏è –†–∞–∑–±–∏–ª–∏ –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ {CHUNK_SIZE} —Å–∏–º–≤–æ–ª–æ–≤")

        insert_index = 1
        for i, chunk in enumerate(chunks):
            docs_service.documents().batchUpdate(documentId=DOC_ID, body={
                "requests": [{
                    "insertText": {
                        "location": {"index": insert_index},
                        "text": chunk
                    }
                }]
            }).execute()
            insert_index += len(chunk)
            print(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω —á–∞–Ω–∫ {i + 1}/{len(chunks)}")

        print(f"üèÅ –í—Å–µ –∑–≤–æ–Ω–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã. –û–±–Ω–æ–≤–ª—è–µ–º last_run: {max_ts}")
        save_last_run(max_ts)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –≤ Google Doc: {e}")

if __name__ == "__main__":
    main()
